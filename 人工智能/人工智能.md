# 1

## 术语

监督学习（有确定的结果）
无监督学习（无确定的结果）

数据集

* 训练集
* 测试集
* 验证集

损失函数
代价函数(cost funcation) J 是整个训练集的函数
梯度下降(lose funcation) L 只针对一个样本数据
学习率
矢量化
欠拟合（高偏差）
泛化
过拟合（高方差）

## 机器学习

### 监督学习

#### 线性回归

![images](./images/屏幕截图%202023-10-13%20182544.png)
![images](./images/屏幕截图%202023-10-13%20182900.png)
![images](./images/屏幕截图%202023-10-13%20204302.png)
![images](./images/屏幕截图%202023-10-13%20205543.png)
![images](./images/屏幕截图%202023-10-13%20210418.png)
![images](./images/屏幕截图%202023-10-13%20211321.png)
![images](./images/屏幕截图%202023-10-13%20211834.png)
![images](./images/屏幕截图%202023-10-13%20212428.png)
![images](./images/屏幕截图%202023-10-13%20212630.png)
![images](./images/屏幕截图%202023-10-13%20212924.png)
![images](./images/屏幕截图%202023-10-13%20213117.png)
![images](./images/屏幕截图%202023-10-13%20214812.png)
![images](./images/屏幕截图%202023-10-13%20215047.png)

**梯度下降**
![images](./images/屏幕截图%202023-10-14%20124842.png)
![images](./images/屏幕截图%202023-10-14%20125417.png)
![images](./images/屏幕截图%202023-10-14%20131327.png)
![images](./images/屏幕截图%202023-10-14%20133651.png)
![images](./images/屏幕截图%202023-10-14%20134226.png)
![images](./images/屏幕截图%202023-10-14%20140227.png)
![images](./images/屏幕截图%202023-10-14%20140859.png)
![images](./images/屏幕截图%202023-10-14%20141130.png)
![images](./images/屏幕截图%202023-10-14%20142752.png)

**导数推导**
![images](./images/屏幕截图%202023-10-14%20143109.png)
![images](./images/屏幕截图%202023-10-14%20143308.png)

**线性回归模型的代价函数是一个凸函数（convex function）一定会收敛到最小值**
![images](./images/屏幕截图%202023-10-14%20143619.png)
![images](./images/屏幕截图%202023-10-14%20144904.png)

#### 多维特征

![images](./images/屏幕截图%202023-10-14%20171217.png)
![images](./images/屏幕截图%202023-10-14%20171502.png)
![images](./images/屏幕截图%202023-10-14%20171935.png)

**矢量化**
![images](./images/屏幕截图%202023-10-14%20172756.png)
![images](./images/屏幕截图%202023-10-14%20181436.png)
![images](./images/屏幕截图%202023-10-14%20181815.png)

**多元线性回归梯度下降**
![images](./images/屏幕截图%202023-10-14%20182324.png)
![images](./images/屏幕截图%202023-10-14%20182711.png)
![images](./images/屏幕截图%202023-10-14%20183001.png)
![images](./images/屏幕截图%202023-10-14%20185222.png)

特征缩放
![images](./images/屏幕截图%202023-10-14%20185432.png)
![images](./images/屏幕截图%202023-10-14%20185749.png)

**特征缩放的实现**

![images](./images/屏幕截图%202023-10-14%20193408.png)
方法一：每个特征除每个特征的最大值

![images](./images/屏幕截图%202023-10-14%20194123.png)
方法二：执行均值归一化

![images](./images/屏幕截图%202023-10-14%20194451.png)
方法三：执行Z分数归一化

![images](./images/屏幕截图%202023-10-14%20194728.png)
可能需要特征缩放的数据

**判断梯度下降是否收敛**

![images](./images/屏幕截图%202023-10-14%20195430.png)
通过迭代次数和代价函数的图像观察或者设定阈值

![images](./images/屏幕截图%202023-10-14%20200506.png)
![images](./images/屏幕截图%202023-10-14%20200733.png)
如何选择合适的学习率

**特征工程**

![images](./images/屏幕截图%202023-10-14%20202133.png)
通过对问题的了解修改特征

![images](./images/屏幕截图%202023-10-14%20202512.png)
![images](./images/屏幕截图%202023-10-14%20202641.png)
修改特征的具体方法

#### 分类算法-逻辑回归

![images](./images/屏幕截图%202023-10-15%20183627.png)
![images](./images/屏幕截图%202023-10-15%20184333.png)

**逻辑回归**

![images](./images/屏幕截图%202023-10-15%20195853.png)
sigmoid函数和logistic函数

![images](./images/屏幕截图%202023-10-15%20200222.png)
将线性回归变为逻辑回归

![images](./images/屏幕截图%202023-10-15%20200617.png)
逻辑回归公式

**决策边界**

![images](./images/屏幕截图%202023-10-16%20144948.png)
![images](./images/屏幕截图%202023-10-16%20145302.png)
线形的决策边界

![images](./images/屏幕截图%202023-10-16%20145724.png)
非线形的决策边界

![images](./images/屏幕截图%202023-10-16%20150324.png)
通过特征工程可以得到复杂的决策边界

**逻辑回归中的代价函数**

![images](./images/屏幕截图%202023-10-16%20151518.png)
直接使用最小二乘法做损失函数

![images](./images/屏幕截图%202023-10-16%20152157.png)
真实值为1时的损失函数

![images](./images/屏幕截图%202023-10-16%20152542.png)
真实值为0时的损失函数

当模型预测的值和实际的结果误差越大，损失就越大

![images](./images/屏幕截图%202023-10-16%20154332.png)
简化的损失函数

![images](./images/屏幕截图%202023-10-16%20154619.png)
简化的代价函数

![images](./images/屏幕截图%202023-10-17%20202702.png)
![images](./images/屏幕截图%202023-10-17%20203224.png)
逻辑回归实现梯度下降

**过拟合问题**

![images](./images/屏幕截图%202023-10-17%20204450.png)
![images](./images/屏幕截图%202023-10-17%20204638.png)

**解决过拟合问题**

![images](./images/屏幕截图%202023-10-18%20175028.png)
收集更多训练数据

![images](./images/屏幕截图%202023-10-18%20175621.png)
选择合适的特征

![images](./images/屏幕截图%202023-10-18%20175924.png)
通过减小w的值减小特征对模型的影响避免过拟合

**正则化**

![images](./images/屏幕截图%202023-10-18%20182018.png)
![images](./images/屏幕截图%202023-10-18%20182457.png)
![images](./images/屏幕截图%202023-10-18%20190311.png)
在代价函数上加入正则项当正则化率大时图像近乎于一条直线
当正则化率小时图像会过拟合
选择合适的正则化率可以减少$\vec{w}$的的值避免过拟合

**用于线性回归的正则方法**

![images](./images/屏幕截图%202023-10-18%20201113.png)
![images](./images/屏幕截图%202023-10-18%20203514.png)
![images](./images/屏幕截图%202023-10-18%20203821.png)

**用于逻辑回归的正则方法**

![images](./images/屏幕截图%202023-10-18%20204602.png)
![images](./images/屏幕截图%202023-10-18%20204930.png
)
![images]()

### 无监督学习
