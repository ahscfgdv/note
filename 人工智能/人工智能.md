**# 1

## 术语

监督学习（有确定的结果）
无监督学习（无确定的结果）

数据集

* 训练集
* 测试集
* 验证集

损失函数
代价函数(cost funcation) J 是整个训练集的函数
梯度下降(lose funcation) L 只针对一个样本数据
学习率
矢量化
欠拟合（高偏差）
泛化
过拟合（高方差）

## 机器学习

### 监督学习

#### 线性回归

![images](./images/屏幕截图%202023-10-13%20182544.png)
![images](./images/屏幕截图%202023-10-13%20182900.png)
![images](./images/屏幕截图%202023-10-13%20204302.png)
![images](./images/屏幕截图%202023-10-13%20205543.png)
![images](./images/屏幕截图%202023-10-13%20210418.png)
![images](./images/屏幕截图%202023-10-13%20211321.png)
![images](./images/屏幕截图%202023-10-13%20211834.png)
![images](./images/屏幕截图%202023-10-13%20212428.png)
![images](./images/屏幕截图%202023-10-13%20212630.png)
![images](./images/屏幕截图%202023-10-13%20212924.png)
![images](./images/屏幕截图%202023-10-13%20213117.png)
![images](./images/屏幕截图%202023-10-13%20214812.png)
![images](./images/屏幕截图%202023-10-13%20215047.png)

**梯度下降**
![images](./images/屏幕截图%202023-10-14%20124842.png)
![images](./images/屏幕截图%202023-10-14%20125417.png)
![images](./images/屏幕截图%202023-10-14%20131327.png)
![images](./images/屏幕截图%202023-10-14%20133651.png)
![images](./images/屏幕截图%202023-10-14%20134226.png)
![images](./images/屏幕截图%202023-10-14%20140227.png)
![images](./images/屏幕截图%202023-10-14%20140859.png)
![images](./images/屏幕截图%202023-10-14%20141130.png)
![images](./images/屏幕截图%202023-10-14%20142752.png)

**导数推导**
![images](./images/屏幕截图%202023-10-14%20143109.png)
![images](./images/屏幕截图%202023-10-14%20143308.png)

**线性回归模型的代价函数是一个凸函数（convex function）一定会收敛到最小值**
![images](./images/屏幕截图%202023-10-14%20143619.png)
![images](./images/屏幕截图%202023-10-14%20144904.png)

#### 多维特征

![images](./images/屏幕截图%202023-10-14%20171217.png)
![images](./images/屏幕截图%202023-10-14%20171502.png)
![images](./images/屏幕截图%202023-10-14%20171935.png)

**矢量化**
![images](./images/屏幕截图%202023-10-14%20172756.png)
![images](./images/屏幕截图%202023-10-14%20181436.png)
![images](./images/屏幕截图%202023-10-14%20181815.png)

**多元线性回归梯度下降**
![images](./images/屏幕截图%202023-10-14%20182324.png)
![images](./images/屏幕截图%202023-10-14%20182711.png)
![images](./images/屏幕截图%202023-10-14%20183001.png)
![images](./images/屏幕截图%202023-10-14%20185222.png)

特征缩放
![images](./images/屏幕截图%202023-10-14%20185432.png)
![images](./images/屏幕截图%202023-10-14%20185749.png)

**特征缩放的实现**

![images](./images/屏幕截图%202023-10-14%20193408.png)
方法一：每个特征除每个特征的最大值

![images](./images/屏幕截图%202023-10-14%20194123.png)
方法二：执行均值归一化

![images](./images/屏幕截图%202023-10-14%20194451.png)
方法三：执行Z分数归一化

![images](./images/屏幕截图%202023-10-14%20194728.png)
可能需要特征缩放的数据

**判断梯度下降是否收敛**

![images](./images/屏幕截图%202023-10-14%20195430.png)
通过迭代次数和代价函数的图像观察或者设定阈值

![images](./images/屏幕截图%202023-10-14%20200506.png)
![images](./images/屏幕截图%202023-10-14%20200733.png)
如何选择合适的学习率

**特征工程**

![images](./images/屏幕截图%202023-10-14%20202133.png)
通过对问题的了解修改特征

![images](./images/屏幕截图%202023-10-14%20202512.png)
![images](./images/屏幕截图%202023-10-14%20202641.png)
修改特征的具体方法

#### 分类算法-逻辑回归

![images](./images/屏幕截图%202023-10-15%20183627.png)
![images](./images/屏幕截图%202023-10-15%20184333.png)

**逻辑回归**

![images](./images/屏幕截图%202023-10-15%20195853.png)
sigmoid函数和logistic函数

![images](./images/屏幕截图%202023-10-15%20200222.png)
将线性回归变为逻辑回归

![images](./images/屏幕截图%202023-10-15%20200617.png)
逻辑回归公式

**决策边界**

![images](./images/屏幕截图%202023-10-16%20144948.png)
![images](./images/屏幕截图%202023-10-16%20145302.png)
线形的决策边界

![images](./images/屏幕截图%202023-10-16%20145724.png)
非线形的决策边界

![images](./images/屏幕截图%202023-10-16%20150324.png)
通过特征工程可以得到复杂的决策边界

**逻辑回归中的代价函数**

![images](./images/屏幕截图%202023-10-16%20151518.png)
直接使用最小二乘法做损失函数

![images](./images/屏幕截图%202023-10-16%20152157.png)
真实值为1时的损失函数

![images](./images/屏幕截图%202023-10-16%20152542.png)
真实值为0时的损失函数

当模型预测的值和实际的结果误差越大，损失就越大

![images](./images/屏幕截图%202023-10-16%20154332.png)
简化的损失函数

![images](./images/屏幕截图%202023-10-16%20154619.png)
简化的代价函数

![images](./images/屏幕截图%202023-10-17%20202702.png)
![images](./images/屏幕截图%202023-10-17%20203224.png)
逻辑回归实现梯度下降

**过拟合问题**

![images](./images/屏幕截图%202023-10-17%20204450.png)
![images](./images/屏幕截图%202023-10-17%20204638.png)

**解决过拟合问题**

![images](./images/屏幕截图%202023-10-18%20175028.png)
收集更多训练数据

![images](./images/屏幕截图%202023-10-18%20175621.png)
选择合适的特征

![images](./images/屏幕截图%202023-10-18%20175924.png)
通过减小w的值减小特征对模型的影响避免过拟合

**正则化**

![images](./images/屏幕截图%202023-10-18%20182018.png)
![images](./images/屏幕截图%202023-10-18%20182457.png)
![images](./images/屏幕截图%202023-10-18%20190311.png)
在代价函数上加入正则项当正则化率大时图像近乎于一条直线
当正则化率小时图像会过拟合
选择合适的正则化率可以减少$\vec{w}$的的值避免过拟合

**用于线性回归的正则方法**

![images](./images/屏幕截图%202023-10-18%20201113.png)
![images](./images/屏幕截图%202023-10-18%20203514.png)
![images](./images/屏幕截图%202023-10-18%20203821.png)

**用于逻辑回归的正则方法**

![images](./images/屏幕截图%202023-10-18%20204602.png)
![images](./images/屏幕截图%202023-10-18%20204930.png)

#### 神经网络

![images](./images/屏幕截图%202023-10-19%20153141.png)
神经网络的发展

![images](./images/屏幕截图%202023-10-19%20154742.png)
简单类比生物神经元和神经网络

![images](./images/屏幕截图%202023-10-19%20155027.png)
神经网络发展的原因

![images](./images/屏幕截图%202023-10-19%20162314.png)
建议神经网络示例

![images](./images/屏幕截图%202023-10-19%20163243.png)
![images](./images/屏幕截图%202023-10-19%20164018.png)
![images](./images/屏幕截图%202023-10-19%20164541.png)
有多个隐藏层的神经网络

**举例图像识别**

![images](./images/屏幕截图%202023-10-19%20170222.png)
![images](./images/屏幕截图%202023-10-19%20170352.png)

**神经网络中的网络层**

![images](./images/屏幕截图%202023-10-19%20180507.png)
第一层

![images](./images/屏幕截图%202023-10-19%20180810.png)
第二层

![images](./images/屏幕截图%202023-10-19%20180925.png)
结果

**更复杂的神经网络**

![images](./images/屏幕截图%202023-10-19%20203228.png)
![images](./images/屏幕截图%202023-10-19%20203712.png)

**前向传播**

![images](./images/屏幕截图%202023-10-19%20204048.png)
![images](./images/屏幕截图%202023-10-19%20204144.png)
![images](./images/屏幕截图%202023-10-19%20204254.png)
![images](./images/屏幕截图%202023-10-21%20125313.png)
手写数字识别

**简易代码实现(基于TensorFlow)**

![images](./images/屏幕截图%202023-10-21%20124645.png)
![images](./images/屏幕截图%202023-10-21%20125122.png)

**TensorFlow中的数据格式**

![images](./images/屏幕截图%202023-10-21%20135003.png)
![images](./images/屏幕截图%202023-10-21%20135233.png)
![images](./images/屏幕截图%202023-10-21%20135523.png)
![images](./images/屏幕截图%202023-10-21%20135627.png)

**搭建简易神经网络**

![images](./images/屏幕截图%202023-10-21%20141040.png)
![images](./images/屏幕截图%202023-10-21%20150001.png)
![images](./images/屏幕截图%202023-10-21%20150552.png)

**单个网络层上的前向传播**

![images](./images/屏幕截图%202023-10-21%20151939.png)
![images](./images/屏幕截图%202023-10-21%20171303.png)

**强人工智能**

![images](./images/屏幕截图%202023-10-21%20171808.png)
ANI和AGI

**神经网络的矢量化**

![images](./images/屏幕截图%202023-10-21%20212516.png)

**矩阵乘法**

![images](./images/屏幕截图%202023-10-21%20212910.png)
点积和矩阵乘法

![images](./images/屏幕截图%202023-10-21%20213119.png)
向量矩阵乘法

![images](./images/屏幕截图%202023-10-21%20213349.png)
![images](./images/屏幕截图%202023-10-21%20213812.png)
![images](./images/屏幕截图%202023-10-21%20214013.png)
![images](./images/屏幕截图%202023-10-21%20214259.png)
矩阵乘法

![images](./images/屏幕截图%202023-10-21%20214624.png)

#### TensorFlow代码实现

![images](./images/屏幕截图%202023-10-21%20215113.png)
![images](./images/屏幕截图%202023-10-22%20191128.png)
模型训练的步骤

![images](./images/屏幕截图%202023-10-22%20191709.png)
![images](./images/屏幕截图%202023-10-22%20193022.png)
![images](./images/屏幕截图%202023-10-22%20193308.png)
具体实现

**不同的激活函数**

![images](./images/屏幕截图%202023-10-22%20194547.png)
sigmomid and ReLU

![images](./images/屏幕截图%202023-10-22%20194726.png)
常用激活函数

**选择激活函数**

![images](./images/屏幕截图%202023-10-22%20195319.png)
根据输出层的不同选择不同的激活函数

![images](./images/屏幕截图%202023-10-22%20195736.png)
隐藏层通常选择ReLU因为计算更快，梯度下降也快训练效果好

![images](./images/屏幕截图%202023-10-22%20195925.png)

**激活函数的作用**

![images](./images/屏幕截图%202023-10-23%20170310.png)

![images](./images/屏幕截图%202023-10-23%20170530.png)
![images](./images/屏幕截图%202023-10-23%20170756.png)
不使用激活函数多层的神经网络就相当于一层的线性回归

**多分类问题**

![images](./images/屏幕截图%202023-10-23%20171009.png)
![images](./images/屏幕截图%202023-10-23%20171056.png)

**softmax函数**

![images](./images/屏幕截图%202023-10-23%20213235.png)
![images](./images/屏幕截图%202023-10-23%20213835.png)
softmax函数的损失函数

![images](./images/屏幕截图%202023-10-23%20214434.png)
![images](./images/屏幕截图%202023-10-23%20214613.png)

![images](./images/屏幕截图%202023-10-23%20220008.png)
![images](./images/屏幕截图%202023-10-23%20220449.png)
优化代码通过底层计算浮点数的不同使结果更加准确
缺点代码可读性差

![images](./images/屏幕截图%202023-10-23%20220805.png)
![images](./images/屏幕截图%202023-10-23%20220920.png)

**多个输出的分类**

![images](./images/屏幕截图%202023-10-25%20140846.png)
![images](./images/屏幕截图%202023-10-25%20141301.png)
查看途中有没有人，汽车，公交车

**高级优化方法(Adam算法)**

![images](./images/屏幕截图%202023-10-25%20141718.png)
![images](./images/屏幕截图%202023-10-25%20142133.png)
![images](./images/屏幕截图%202023-10-25%20142430.png)
Adam算法可以自动调整学习率，更好的训练模型

**其他网络层类型**

![images](./images/屏幕截图%202023-10-25%20142805.png)
密集层

![images](./images/屏幕截图%202023-10-25%20142949.png)
![images](./images/屏幕截图%202023-10-25%20143200.png)
卷积层

**导数知识**

![images](./images/屏幕截图%202023-10-25%20152219.png)
![images](./images/屏幕截图%202023-10-25%20152531.png)
![images](./images/屏幕截图%202023-10-25%20153728.png)

**计算图**

![images](./images/屏幕截图%202023-10-25%20154022.png)
![images](./images/屏幕截图%202023-10-25%20154624.png)
![images](./images/屏幕截图%202023-10-25%20154726.png)
![images](./images/屏幕截图%202023-10-25%20154942.png)

**大型神经网络**

![images](./images/屏幕截图%202023-10-25%20160406.png)
![images](./images/屏幕截图%202023-10-25%20160617.png)

### 偏差和方差

![images](./images/屏幕截图%202023-10-25%20161146.png)

**模型评估**

![images](./images/屏幕截图%202023-10-25%20175721.png)
![images](./images/屏幕截图%202023-10-25%20175858.png)
![images](./images/屏幕截图%202023-10-25%20180128.png)
![images](./images/屏幕截图%202023-10-25%20180254.png)
![images](./images/屏幕截图%202023-10-25%20180356.png)
![images](./images/屏幕截图%202023-10-25%20180506.png)

**模型选择**

![images](./images/屏幕截图%202023-10-25%20181156.png)
只通过测试集得到的损失函数泛化度不够

![images](./images/屏幕截图%202023-10-25%20181357.png)
通过添加dev test选择模型

![images](./images/屏幕截图%202023-10-25%20181944.png)
![images](./images/屏幕截图%202023-10-25%20182438.png)

**偏差和方差**

![images](./images/屏幕截图%202023-10-26%20144412.png)
高偏差$J_{train}$高
高方差$J_{cv}>J_{train}$

![images](./images/屏幕截图%202023-10-26%20144716.png)
![images](./images/屏幕截图%202023-10-26%20145058.png)

**正则化，偏差和方差**

![images](./images/屏幕截图%202023-10-26%20145506.png)
![images](./images/屏幕截图%202023-10-26%20145914.png)
![images](./images/屏幕截图%202023-10-26%20150105.png)

**实例语音识别**

![images](./images/屏幕截图%202023-10-26%20150940.png)
![images](./images/屏幕截图%202023-10-26%20151129.png)
![images](./images/屏幕截图%202023-10-26%20151435.png)

**学习曲线**

![images](./images/屏幕截图%202023-10-26%20152040.png)
![images](./images/屏幕截图%202023-10-26%20152443.png)
高偏差增加数据集基本没用，模型出现问题

![images](./images/屏幕截图%202023-10-26%20152902.png)
高方差可以增加数据集

**结论**

![images](./images/屏幕截图%202023-10-26%20154038.png)
![images](./images/屏幕截图%202023-10-26%20154526.png)
![images](./images/屏幕截图%202023-10-26%20183204.png)
![images](./images/屏幕截图%202023-10-26%20183340.png)
![images](./images/屏幕截图%202023-10-26%20183607.png)
选择合适的正则化参数可以让神经网络避免过拟合，但是会增加计算量

### 机器学习开发的迭代循环

![images](./images/屏幕截图%202023-10-26%20184720.png)
![images](./images/屏幕截图%202023-10-26%20184841.png)
垃圾邮件示例

**误差分析**

![images](./images/屏幕截图%202023-10-29%20161450.png)
![images](./images/屏幕截图%202023-10-29%20161538.png)
![images](./images/屏幕截图%202023-10-29%20161711.png)

**添加数据**

![images](./images/屏幕截图%202023-10-29%20163128.png)
![images](./images/屏幕截图%202023-10-29%20163233.png)
![images](./images/屏幕截图%202023-10-29%20163450.png)
![images](./images/屏幕截图%202023-10-29%20163659.png)
数据增强修改现有的数据来构建新的数据集

![images](./images/屏幕截图%202023-10-29%20163802.png)
![images](./images/屏幕截图%202023-10-29%20163952.png)
![images](./images/屏幕截图%202023-10-29%20164538.png)
![images](./images/屏幕截图%202023-10-29%20164412.png)
创建新的数据通过原始数据和不同的字体结合生成不同的数据

![images](./images/屏幕截图%202023-10-29%20164752.png)

**迁移学习**

![images](./images/屏幕截图%202023-10-29%20170614.png)
![images](./images/屏幕截图%202023-10-29%20170914.png)
![images](./images/屏幕截图%202023-10-29%20171211.png)

**机器学习项目的完整周期**

![images](./images/屏幕截图%202023-10-29%20180355.png)
![images](./images/屏幕截图%202023-10-29%20180923.png)

**道德相关**

![images](./images/屏幕截图%202023-10-29%20182513.png)
![images](./images/屏幕截图%202023-10-29%20182622.png)
fake video

![images](./images/屏幕截图%202023-10-29%20182916.png)
![images](./images/屏幕截图%202023-10-29%20183055.png)

**倾斜数据集**

![images](./images/屏幕截图%202023-10-29%20184932.png)
![images](./images/屏幕截图%202023-10-29%20185816.png)
对于罕见的数据预测时可能一直预测0，你的模型准确度可能很高，但是缺没有意义，引入其他指标来评估模型
precision：猜的准确率
recall：避免漏检

**精确率和召回率**

![images](./images/屏幕截图%202023-10-29%20190944.png)
![images](./images/屏幕截图%202023-10-29%20191347.png)

### 决策树

**决策树模型**

![images](./images/屏幕截图%202023-10-30%20213303.png)
![images](./images/屏幕截图%202023-10-30%20213659.png)
![images](./images/屏幕截图%202023-10-30%20213819.png)

**学习过程**

![images](./images/屏幕截图%202023-10-30%20214348.png)
![images](./images/屏幕截图%202023-10-30%20214733.png)
![images](./images/屏幕截图%202023-10-30%20214934.png)
![images](./images/屏幕截图%202023-10-30%20215116.png)

**纯度**

![images](./images/屏幕截图%202023-10-30%20215902.png)
引入熵来衡量数据的纯度

![images](./images/屏幕截图%202023-10-30%20220243.png)
规定$0*\log_20 = 0$

**区分信息**

![images](./images/屏幕截图%202023-10-30%20221255.png)
通过熵的减少量选择分支，熵的减少也叫信息增益(information gain)

![images](./images/屏幕截图%202023-10-30%20221522.png)
计算公式

**整合**

![images](./images/屏幕截图%202023-10-31%20183400.png)
![images](./images/屏幕截图%202023-10-31%20184556.png)
递归的思想来构建决策树，通过树的深度设定来避免过拟合

**one-hot编码**

![images](./images/屏幕截图%202023-10-31%20190120.png)
![images](./images/屏幕截图%202023-10-31%20190258.png)
![images](./images/屏幕截图%202023-10-31%20190335.png)
编码规则

![images](./images/屏幕截图%202023-10-31%20190617.png)

**连续的特征值**

![images](./images/屏幕截图%202023-10-31%20190834.png)
![images](./images/屏幕截图%202023-10-31%20191431.png)
遍历计算信息增益，来决定分割数据的阈值

**回归树**

![images](./images/屏幕截图%202023-10-31%20204432.png)
![images](./images/屏幕截图%202023-10-31%20204704.png)
![images](./images/屏幕截图%202023-10-31%20205234.png)
用方差衡量数据的混乱程度，同样计算信息增益来选择分支

**决策树集群**

![images](./images/屏幕截图%202023-10-31%20205619.png)
只训练一个树的话只更改一个数据就会得到不同的决策树，为了是算法更加强大可以训练多个树

![images](./images/屏幕截图%202023-10-31%20205913.png)
让三颗树投票决定最终的结果

**有放回抽样(sampling with replacement)**

![images](./images/屏幕截图%202023-10-31%20210429.png)
![images](./images/屏幕截图%202023-10-31%20210613.png)
有放回抽样可以构建新的训练集

**随机森林**

![images](./images/屏幕截图%202023-10-31%20212451.png)
![images](./images/屏幕截图%202023-10-31%20212704.png)

**XGBoost**

![images](./images/屏幕截图%202023-10-31%20214328.png)
boost优化方法

![images](./images/屏幕截图%202023-10-31%20214529.png)
XGBoost优化方法

![images](./images/屏幕截图%202023-10-31%20214627.png)
使用

**何时使用决策树**

![images](./images/屏幕截图%202023-10-31%20215032.png)
![images](./images/屏幕截图%202023-10-31%20215213.png)

### 无监督学习

![images](./images/屏幕截图%202023-11-02%20170105.png)

**聚类算法**

![images](./images/屏幕截图%202023-11-02%20170316.png)
监督学习：二元分类

![images](./images/屏幕截图%202023-11-02%20170502.png)
无监督学习：聚类算法

![images](./images/屏幕截图%202023-11-02%20170725.png)
聚类算法的应用

**K-means算法**

![images](./images/屏幕截图%202023-11-02%20171124.png)
计算每个点到两个簇心的距离将其分为两类离红色近的或者离蓝色近的

![images](./images/屏幕截图%202023-11-02%20171335.png)
移动簇心使其到每个点的距离最小

![images](./images/屏幕截图%202023-11-02%20171851.png)
重复

**K-means算法具体实现**

![images](./images/屏幕截图%202023-11-02%20191556.png)
![images](./images/屏幕截图%202023-11-02%20191954.png)
![images](./images/屏幕截图%202023-11-02%20192100.png)
特殊情况：有一个簇没有点，那么这个簇会被删除

![images](./images/屏幕截图%202023-11-02%20192355.png)

**优化目标**

![images](./images/屏幕截图%202023-11-02%20194144.png)
重新确定簇心的过程就是代价函数
这个函数也叫失真函数(distortion function)
每个点到与他最近的簇心距离的平均和

![images](./images/屏幕截图%202023-11-02%20194422.png)
![images](./images/屏幕截图%202023-11-02%20194928.png)
![images](./images/屏幕截图%202023-11-02%20195125.png)
$C^{(i)}$:离簇心最近的点的集合 i:簇心的索引
$\mu_k$:簇心

**初始化K-means**

![images](./images/屏幕截图%202023-11-02%20200458.png)
![images](./images/屏幕截图%202023-11-02%20200818.png)
方法一：随机选择数据集中的数据作为簇心

![images](./images/屏幕截图%202023-11-02%20201405.png)
方法二：得到了多个模型后使用代价函数来选择模型

![images](./images/屏幕截图%202023-11-02%20201708.png)

**选择聚类数量**

![images](./images/屏幕截图%202023-11-02%20202447.png)
![images](./images/屏幕截图%202023-11-02%20202639.png)
肘法(不推荐)

![images](./images/屏幕截图%202023-11-02%20202806.png)
通过业务具体分析选择K值

### 异常检测算法

![images](./images/屏幕截图%202023-11-02%20203542.png)
![images](./images/屏幕截图%202023-11-02%20203736.png)
![images](./images/屏幕截图%202023-11-02%20203904.png)
应用示例

**高斯正态分布**

![images](./images/屏幕截图%202023-11-04%20120153.png)
![images](./images/屏幕截图%202023-11-04%20120502.png)
图形的面积为一

![images](./images/屏幕截图%202023-11-04%20121109.png)
应用于异常检测算法

![images](./images/屏幕截图%202023-11-04%20121249.png)

**异常检测算法**

![images](./images/屏幕截图%202023-11-04%20122139.png)
对于多个特征的异常检测

![images](./images/屏幕截图%202023-11-04%20125436.png)
矢量化计算

![images](./images/屏幕截图%202023-11-04%20122737.png)
算法的工作流程

![images](./images/屏幕截图%202023-11-04%20123128.png)
应用示例

**开发和评估异常检测**

![images](./images/屏幕截图%202023-11-04%20124614.png)
通过标记的数据测试模型来决定$\epsilon$

![images](./images/屏幕截图%202023-11-04%20125929.png)
![images](./images/屏幕截图%202023-11-04%20130105.png)
也可以使用精确率和召回率来评估模型

**异常检测对比监督学习**

![images](./images/屏幕截图%202023-11-04%20130651.png)
异常检测更适合预测未出现的异常种类，监督学习度已经出现过的错误可以更好地检测

![images](./images/屏幕截图%202023-11-04%20130946.png)
示例

**选择特征**

![images](./images/屏幕截图%202023-11-04%20131308.png)
![images](./images/屏幕截图%202023-11-04%20131730.png)
![images](./images/屏幕截图%202023-11-04%20131833.png)
![images]()
