# 机器学习

机器学习通常包括以下步骤：

1. **问题定义**：明确定义问题，并确保了解问题的类型（分类、回归、聚类等）以及目标。

2. **数据收集**：收集与问题相关的数据，这可能涉及数据采集、数据库查询、数据清理和数据整合等工作。

3. **数据预处理**：对数据进行清理、转换和整理，包括处理缺失值、异常值、标准化数据、编码分类变量等操作，以便为模型训练做准备。

4. **特征工程**：选择、提取和构建能够代表数据特征的属性。这可能涉及特征选择、降维（如主成分分析）、创建新特征等操作。

5. **模型选择**：根据问题类型和数据特性选择合适的机器学习模型，比如决策树、支持向量机、神经网络等。

6. **模型训练**：使用训练数据对选定的模型进行训练，模型根据数据调整参数以最小化预测错误或损失函数。

7. **模型评估**：使用测试数据或交叉验证等技术评估模型性能，通过各种指标（如准确率、精确率、召回率等）评估模型效果。

8. **模型调优**：根据评估结果对模型进行调整和优化，可能包括调整超参数、改进特征工程等。

9. **模型部署**：将训练好的模型应用于新的数据集或实际场景中，进行预测或决策。

10. **模型监控与维护**：对部署的模型进行监控，确保其在生产环境中的性能，并进行必要的维护和更新，以适应新的数据和场景。

这些步骤通常构成了一个基本的机器学习流程，但实际应用中可能会因项目需求和特定情况而有所不同。

机器学习的步骤：

- 样本标注得到数据集
- 通过算法学习数据集的规律得到模型
- 通过模型完成预测

## 数据预处理

在机器学习中，数据预处理是非常重要的环节，常用的数据预处理方法包括：

1. **缺失值处理**：
   - 删除含有缺失值的样本或特征。
   - 插值法（均值、中位数、众数填充）。
   - 使用机器学习算法进行缺失值的预测填充。

2. **异常值处理**：
   - 检测并根据业务逻辑或数据分布移除或修正异常值。
   - 使用统计方法（如标准差或箱线图）识别异常值，并进行处理。

3. **特征标准化和归一化**：
   - 标准化（Standardization）：将数据转换为均值为0，方差为1的标准正态分布。
   - 归一化（Normalization）：将数据缩放到0和1之间，使数据落入特定范围。

4. **分类变量编码**：
   - One-Hot 编码：将分类变量转换为二进制形式。
   - 标签编码：将分类变量映射为整数值。

5. **特征工程**：
   - 特征选择：选择最相关或最具信息量的特征。
   - 主成分分析（PCA）：降维以减少特征数量，保留最重要的特征。

6. **数据平衡处理**（对于不平衡数据集）：
   - 过采样：增加少数类样本。
   - 欠采样：减少多数类样本。
   - 合成少数类样本：使用合成方法生成新的少数类样本。

7. **时间序列处理**：
   - 平滑处理：利用滑动窗口或移动平均来平滑时间序列数据。
   - 时间特征提取：从时间戳中提取年份、季节、月份等特征。

8. **文本数据处理**：
   - 分词：将文本拆分成单词或词组。
   - 词袋模型：将文本转换为向量表示。
   - TF-IDF（词频-逆文档频率）：衡量词语在文档中的重要性。

不同的预处理方法适用于不同类型的数据和机器学习问题，选择合适的方法通常取决于数据的特点和建模需求。

### 缺失值处理

### 分类变量编码

## 监督学习

### KNN分类算法(K近邻)

核心思想：一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，k通常取值为1-100

K值：K值过大会导致过拟合，K值过小会导致欠拟合

距离的度量：通过距离的大小来判断两个样本的相似程度
判断距离的公式有：欧式距离，曼哈顿距离，切比雪夫距离，闵可夫斯基距离，马氏距离

欧式距离：$\rho=\sqrt{(x_2-x_1)^2-(y_2-y_1)^2}$

### 决策树

### 回归算法

回归分析(regression analysis)是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。回归属于监督学习方法。

#### 线性回归

##### 核心算法：线性回归方程

线性回归是一种建立连续型输出变量与一个或多个输入变量之间关系的线性模型。其表达式可以表示为：

\[ \hat{y} = w_1x_1 + w_2x_2 + \ldots + w_nx_n + b\]

其中：

- \(\hat{y}\) 是预测的输出变量值。
- \(x_1, x_2, \ldots, x_n\) 是输入变量。
- \(w_1, w_2, \ldots, w_n\) 是回归系数（权重），\(b\) 是截距。

在简单线性回归中，只涉及一个输入变量 \(x\) 和一个输出变量 \(y\) 的情况下，方程可以简化为：

\[ \hat{y} =  w_1x + b\]

这里：

- \(b\) 是截距（表示当 \(x=0\) 时的 \(y\) 值）。
- \(w_1\) 是斜率（表示 \(x\) 变化时 \(y\) 的变化量）。

模型的目标是找到最合适的 \(b\) 和 \(w_1\)，使得模型预测的值 \(\hat{y}\) 尽可能接近实际观测到的 \(y\) 值。通常使用最小化损失函数的方法来确定这些系数，例如最小二乘法，即最小化实际值与预测值之间的平方差。

这个线性方程能够简单地描述输入和输出之间的线性关系，但对于非线性关系，其他回归模型可能更适合。

##### 损失函数

线性回归模型通常使用最小化损失函数的方法（梯度下降）来确定最佳拟合的回归系数。最常见的损失函数是**最小二乘法（Ordinary Least Squares，OLS）**，它的损失函数是预测值与真实值之间差值的平方和。

对于简单线性回归（单个输入变量），最小二乘法的损失函数可以表示为：

\[ \text{Loss} = \sum_{i=1}^{n} (y_i - (b + w_1x_i))^2 \]

其中：

- \(n\) 是样本数量。
- \(y_i\) 是第 \(i\) 个样本的实际输出值。
- \(x_i\) 是第 \(i\) 个样本的输入值。
- \(b\) 是截距。
- \(w_1\) 是斜率。

最小化这个损失函数的过程就是找到最佳的 \(b\) 和 \(w_1\)，使得损失函数最小化。这可以通过不同的优化算法（如梯度下降）来实现。

对于多元线性回归（多个输入变量），最小二乘法的损失函数变为：

\[ \text{Loss} = \sum_{i=1}^{n} (y_i - (b + w_1x_{i1} + w_2x_{i2} + \ldots + w_mx_{im}))^2 \]

其中 \(m\) 是输入变量的数量。

最小二乘法在实践中很常用，因为它有解析解，可以直接通过数学公式计算出最优的回归系数，不需要迭代优化。但是，对于大规模数据集或复杂模型，可能会有其他更高级的优化技术被用于最小化损失函数。

##### 梯度下降

简单线性回归中，梯度下降是一种优化算法，用于最小化损失函数，以找到最佳的回归系数 \(b\) 和 \(w_1\)。梯度下降的目标是沿着损失函数梯度的反方向逐步调整参数，直至达到损失函数的最小值。

对于简单线性回归的最小二乘法损失函数：

\[ \text{Loss} = \sum_{i=1}^{n} (y_i - (b + w_1x_i))^2 \]

梯度下降的步骤如下：

1. **初始化参数 \(b\) 和 \(w_1\)**：可以随机初始化或使用零值开始。

2. **计算损失函数关于参数的梯度**：对损失函数进行求导，得到关于 \(b\) 和 \(w_1\) 的梯度。

3. **更新参数**：沿着梯度的反方向调整参数值，更新 \(b\) 和 \(w_1\)。更新公式如下：
其中 \(\alpha\) 是学习率，控制每次参数更新的步长。

\[ b = b - \alpha \frac{\partial \text{Loss}}{\partial b} \]
\[ w_1 = w_1 - \alpha \frac{\partial \text{Loss}}{\partial w_1} \]

4. **重复迭代步骤2和步骤3**：直至达到指定的迭代次数或损失函数收敛到足够小的值。

关于梯度的具体计算会涉及损失函数的偏导数。对于最小二乘法的简单线性回归，梯度计算如下：

\[
\frac{\partial \text{Loss}}{\partial b} = -2 \sum_{i=1}^{n} (y_i - (b + w_1x_i))
\]

\[
\frac{\partial \text{Loss}}{\partial w_1} = -2 \sum_{i=1}^{n} x_i(y_i - (b + w_1x_i))
\]

梯度下降算法中的学习率 \(\alpha\) 的选择很重要。过小的学习率可能导致收敛速度缓慢，而过大的学习率可能导致震荡甚至无法收敛。所以，选择一个合适的学习率是调整梯度下降算法的关键部分。

#### 逻辑回归

### 推荐算法

### 支持向量机

## 无监督学习

### K-Means(K均值聚类)

**训练过程**

第一步：选择k个点作为初始簇心

第二步：计算每个点到每个簇心的距离，将每个点划分到距离最近的簇中

第三步：跟新簇心位置

第四步：重复第二步和第三步，直到簇心位置基本不变

**性能评估**

聚类算法是无监督算法性能比较低需要完成性能评估

常用的评价方法

- 外部有效性评价
- 内部有效性评价
- 相关性测试评价

K-Means目标函数

假设数据集X包含n个数据点，需要划分k个类聚类中心的集合用U表示

$J=\sum_{c=1}^{k}\sum_{i=1}^{n}||x_i-u_c||^2$

**科学的确定k值**

1. 通过经验判断
2. 可视化数据人工判断
3. 通过肘法画出J关于k值的图可以明显看见拐点确定k值
**SSE(误差平方和)**$\sum_{i=1}^n(y_i-\widehat{y})$其实和函数J是一样的

## 强化学习

## 深度学习

## 性能评估
