# 机器学习

机器学习通常包括以下步骤：

1. **问题定义**：明确定义问题，并确保了解问题的类型（分类、回归、聚类等）以及目标。

2. **数据收集**：收集与问题相关的数据，这可能涉及数据采集、数据库查询、数据清理和数据整合等工作。

3. **数据预处理**：对数据进行清理、转换和整理，包括处理缺失值、异常值、标准化数据、编码分类变量等操作，以便为模型训练做准备。

4. **特征工程**：选择、提取和构建能够代表数据特征的属性。这可能涉及特征选择、降维（如主成分分析）、创建新特征等操作。

5. **模型选择**：根据问题类型和数据特性选择合适的机器学习模型，比如决策树、支持向量机、神经网络等。

6. **模型训练**：使用训练数据对选定的模型进行训练，模型根据数据调整参数以最小化预测错误或损失函数。

7. **模型评估**：使用测试数据或交叉验证等技术评估模型性能，通过各种指标（如准确率、精确率、召回率等）评估模型效果。

8. **模型调优**：根据评估结果对模型进行调整和优化，可能包括调整超参数、改进特征工程等。

9. **模型部署**：将训练好的模型应用于新的数据集或实际场景中，进行预测或决策。

10. **模型监控与维护**：对部署的模型进行监控，确保其在生产环境中的性能，并进行必要的维护和更新，以适应新的数据和场景。

这些步骤通常构成了一个基本的机器学习流程，但实际应用中可能会因项目需求和特定情况而有所不同。

机器学习的步骤：

- 样本标注得到数据集
- 通过算法学习数据集的规律得到模型
- 通过模型完成预测

## 数据预处理

在机器学习中，数据预处理是非常重要的环节，常用的数据预处理方法包括：

1. **缺失值处理**：
   - 删除含有缺失值的样本或特征。
   - 插值法（均值、中位数、众数填充）。
   - 使用机器学习算法进行缺失值的预测填充。

2. **异常值处理**：
   - 检测并根据业务逻辑或数据分布移除或修正异常值。
   - 使用统计方法（如标准差或箱线图）识别异常值，并进行处理。

3. **特征标准化和归一化**：
   - 标准化（Standardization）：将数据转换为均值为0，方差为1的标准正态分布。
   - 归一化（Normalization）：将数据缩放到0和1之间，使数据落入特定范围。

4. **分类变量编码**：
   - One-Hot 编码：将分类变量转换为二进制形式。
   - 标签编码：将分类变量映射为整数值。

5. **特征工程**：
   - 特征选择：选择最相关或最具信息量的特征。
   - 主成分分析（PCA）：降维以减少特征数量，保留最重要的特征。

6. **数据平衡处理**（对于不平衡数据集）：
   - 过采样：增加少数类样本。
   - 欠采样：减少多数类样本。
   - 合成少数类样本：使用合成方法生成新的少数类样本。

7. **时间序列处理**：
   - 平滑处理：利用滑动窗口或移动平均来平滑时间序列数据。
   - 时间特征提取：从时间戳中提取年份、季节、月份等特征。

8. **文本数据处理**：
   - 分词：将文本拆分成单词或词组。
   - 词袋模型：将文本转换为向量表示。
   - TF-IDF（词频-逆文档频率）：衡量词语在文档中的重要性。

不同的预处理方法适用于不同类型的数据和机器学习问题，选择合适的方法通常取决于数据的特点和建模需求。

### 缺失值处理

### 分类变量编码

## 监督学习

### KNN分类算法(K近邻)

#### 核心思想
 
核心思想：一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，k通常取值为1-100

**K值**：
K值过大会导致过拟合
K值过小会导致欠拟合

**优点**:
- 精度高
- 对异常值不敏感
- 无数据输入假定

**缺点**:
- 计算复杂度高
- 空间复杂度高

#### 距离的判断

距离的度量：通过距离的大小来判断两个样本的相似程度
判断距离的公式有：欧式距离，曼哈顿距离，切比雪夫距离，闵可夫斯基距离，马氏距离

**欧式距离**:\(\rho=\sqrt{(x_2-x_1)^2-(y_2-y_1)^2}\)

**曼哈顿距离**:在二维空间内，两个点之间的曼哈顿距离为它们横坐标之差的绝对值与纵坐标之差的绝对值之和
\[\rho=|x_1-x_2|+|y_1-y_2|\]

**切比雪夫距离**:在二维空间内，两个点之间的切比雪夫距离为它们横坐标之差的绝对值与纵坐标之差的绝对值的最大值

\[\rho = max(|x_1-x_2|,|y_1-y_2|)\]

****

**n维特征拓展**

**LP距离**:是一种衡量两点之间距离的方式，可以用来度量向量空间中点的差异。这个距离的公式如下：

如果有两个点 \(p = (p_1, p_2, \dots, p_n)\) 和 \(q = (q_1, q_2, \dots, q_n)\)，它们之间的LP距离定义为：

\[ \left( \sum_{i=1}^{n} |p_i - q_i|^p \right)^{\frac{1}{p}} \]

在这个公式中，\(p\) 是一个参数，可以是任何实数。当 \(p = 1\) 时，我们得到的是曼哈顿距离（Manhattan Distance），\(p = 2\) 时是欧几里得距离（Euclidean Distance）。

#### 分类决策规则

分类结果的确定往往采用**多数表决**原则，即由输入实例的k个最邻近的训练实例中的多数类决定输入实例的类别。


### 决策树

#### 决策树介绍

决策树是一树状结构，它的每一个叶节点对应着一个分类，非叶节点对应着在某个属性上的划分，根据样本在该属性上的不同取值将其划分成若干个子集。对于非纯的叶节点，多数类的标号给出到达这个节点的样本所属的类。构造决策树的核心问题是在每一步如何选择适当的属性对样本做拆分。对一个分类问题，从已知类标记的训练样本中学习并构造出决策树是一个自上而下，分而治之的过程。

#### 信息增益

**信息熵**

信息熵是衡量数据不确定性的一种方式，在决策树算法中用于选择最优特征进行数据分割。信息熵的计算公式如下所示：

假设有一个包含类别标签的数据集，其中包含 \(N\) 个样本，每个样本属于不同的类别。令 \(p_i\) 表示第 \(i\) 个类别的样本在数据集中的比例。

信息熵的计算公式为：

\[ \text{Entropy(S)} = - \sum_{i=1}^{k} p_i \cdot \log_{2}(p_i) \]

其中，\(k\) 是类别的个数。


信息熵的值越高，表示数据集的不确定性越大，即数据越混乱；而信息熵越低，则表示数据集的纯度越高，即数据更加有序。在决策树中，通过计算信息熵来选择最优的特征进行数据分割，以达到信息增益最大化的目的。

****

**信息增益**

信息增益是在决策树算法中用来衡量某个特征对于分类任务的重要性。它衡量的是使用某个特征对数据进行划分后，信息熵的减少程度。信息增益的计算涉及原始数据集的信息熵和特征划分后的条件熵。

### 计算步骤：
1. **计算数据集的信息熵（Entropy(S)）**：使用数据集中各类别的比例计算整个数据集的信息熵。
2. **对数据集按特征进行划分**：将数据集按照特征的不同取值划分成若干子集。
3. **计算条件熵（Conditional Entropy）**：针对每个特征值的子集，分别计算其信息熵，并按子集的样本数加权求和得到条件熵。
4. **计算信息增益**：用原始数据集的信息熵减去条件熵，得到信息增益。

### 公式表示：
设有数据集 \(S\)，特征 \(A\) 有 \(n\) 个取值，对应于 \(n\) 个子集 \(S_1, S_2, \dots, S_n\)，每个子集包含的样本数分别为 \(|S_1|, |S_2|, \dots, |S_n|\)。

信息增益的计算公式如下：

\[ \text{Gain}(S, A) = \text{Entropy}(S) - \sum_{i=1}^{n} \frac{|S_i|}{|S|} \cdot \text{Entropy}(S_i) \]

其中，
- \(\text{Entropy}(S)\) 表示数据集 \(S\) 的信息熵。
- \(\text{Entropy}(S_i)\) 表示按特征 \(A\) 的第 \(i\) 个取值进行划分后的子集 \(S_i\) 的信息熵。

**计算信息增益的步骤**：
1. 计算原始数据集的信息熵（\(\text{Entropy}(S)\)）。
2. 对于每个特征值，计算其对应子集的信息熵（\(\text{Entropy}(S_i)\)）。
3. 根据公式计算信息增益（\(\text{Gain}(S, A)\)）。
4. 选择信息增益最大的特征作为当前节点的划分特征。

**原始数据的熵减去按某种特征分化化后加权的熵**

信息增益越大，表示使用该特征进行划分能够更好地降低数据集的不确定性，因此在决策树的构建中会优先选择信息增益大的特征进行节点的划分。

****

**增益率**

增益率（Gain Ratio）是决策树算法中用于特征选择的一种指标，它是信息增益（Information Gain）的一种调整形式，考虑了特征取值数目对信息增益的影响，以避免偏向于取值较多的特征。

增益率的计算公式为：

\[ \text{Gain Ratio}(S, A) = \frac{\text{Gain}(S, A)}{\text{SplitInfo}(S, A)} \]

其中，
- \(\text{Gain}(S, A)\) 表示特征 \(A\) 对数据集 \(S\) 的信息增益。
- \(\text{SplitInfo}(S, A)\) 是特征 \(A\) 的分裂信息（Split Information）。

分裂信息是特征 \(A\) 对数据集 \(S\) 进行划分所需的信息量，计算方式如下：

\[ \text{SplitInfo}(S, A) = - \sum_{i=1}^{n} \frac{|S_i|}{|S|} \cdot \log_2 \left(\frac{|S_i|}{|S|}\right) \]

其中，
- \(|S_i|\) 是根据特征 \(A\) 的第 \(i\) 个取值进行划分后的子集大小。
- \(|S|\) 是原始数据集的大小。
- \(n\) 是特征 \(A\) 取值的个数。

增益率考虑了信息增益的同时，通过分裂信息对信息增益进行了调整，惩罚了取值数目较多的特征，使得在特征选择时更加公平。在决策树算法中，除了考虑信息增益外，增益率也可以作为一种特征选择的指标来避免过度偏好取值较多的特征。

#### 剪枝

决策树的剪枝是为了防止过拟合，提高模型的泛化能力而进行的一种策略。剪枝可以分为预剪枝（Pre-pruning）和后剪枝（Post-pruning）两种方法。

### 预剪枝（Pre-pruning）：
在树的构建过程中，在节点展开前进行剪枝，即在选择特征进行节点分裂时，预先通过一些条件来判断是否进行分裂。常用的预剪枝条件包括：
- 最大深度：限制树的深度，防止树生长过深。
- 最小样本数：限制节点分裂所需的最小样本数，当节点样本数低于该阈值时停止分裂。
- 叶节点纯度：当节点中样本的类别足够纯时（达到一定的阈值），停止分裂。

预剪枝的优点在于节省计算资源，避免了过度拟合，但可能会导致欠拟合问题，因为提前停止可能阻止了更多分裂，使得树不够复杂。

### 后剪枝（Post-pruning）：
在树的构建完成后，通过对已生成的决策树进行修剪，去除部分分支或节点。具体步骤包括：
1. **自底向上遍历**：从叶节点开始向上回溯。
2. **计算剪枝前后的模型性能**：对于每个内部节点，剪枝后用验证集或交叉验证集评估模型性能。
3. **剪枝决策**：如果剪枝后模型性能不下降或有所提升，则进行剪枝操作，将该节点转为叶节点。

后剪枝相对于预剪枝来说，更灵活，能更细致地调整树的结构，但在大型数据集上可能计算量较大。

剪枝策略的选择通常是一个平衡问题，预剪枝倾向于节省计算资源和避免过拟合，后剪枝则更注重模型的优化和泛化能力。结合两种方法往往能够获得更好的性能。

### 回归算法

回归分析(regression analysis)是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。回归属于监督学习方法。

#### 线性回归

##### 核心算法：线性回归方程

线性回归是一种建立连续型输出变量与一个或多个输入变量之间关系的线性模型。其表达式可以表示为：

\[ \hat{y} = w_1x_1 + w_2x_2 + \ldots + w_nx_n + b\]

其中：

- \(\hat{y}\) 是预测的输出变量值。
- \(x_1, x_2, \ldots, x_n\) 是输入变量。
- \(w_1, w_2, \ldots, w_n\) 是回归系数（权重），\(b\) 是截距。

在简单线性回归中，只涉及一个输入变量 \(x\) 和一个输出变量 \(y\) 的情况下，方程可以简化为：

\[ \hat{y} =  w_1x + b\]

这里：

- \(b\) 是截距（表示当 \(x=0\) 时的 \(y\) 值）。
- \(w_1\) 是斜率（表示 \(x\) 变化时 \(y\) 的变化量）。

模型的目标是找到最合适的 \(b\) 和 \(w_1\)，使得模型预测的值 \(\hat{y}\) 尽可能接近实际观测到的 \(y\) 值。通常使用最小化损失函数的方法来确定这些系数，例如最小二乘法，即最小化实际值与预测值之间的平方差。

这个线性方程能够简单地描述输入和输出之间的线性关系，但对于非线性关系，其他回归模型可能更适合。

##### 损失函数

线性回归模型通常使用最小化损失函数的方法（梯度下降）来确定最佳拟合的回归系数。最常见的损失函数是**最小二乘法（Ordinary Least Squares，OLS）**，它的损失函数是预测值与真实值之间差值的平方和。

对于简单线性回归（单个输入变量），最小二乘法的损失函数可以表示为：

\[ \text{Loss} = \sum_{i=1}^{n} (y_i - (b + w_1x_i))^2 \]

其中：

- \(n\) 是样本数量。
- \(y_i\) 是第 \(i\) 个样本的实际输出值。
- \(x_i\) 是第 \(i\) 个样本的输入值。
- \(b\) 是截距。
- \(w_1\) 是斜率。

最小化这个损失函数的过程就是找到最佳的 \(b\) 和 \(w_1\)，使得损失函数最小化。这可以通过不同的优化算法（如梯度下降）来实现。

对于多元线性回归（多个输入变量），最小二乘法的损失函数变为：

\[ \text{Loss} = \sum_{i=1}^{n} (y_i - (b + w_1x_{i1} + w_2x_{i2} + \ldots + w_mx_{im}))^2 \]

其中 \(m\) 是输入变量的数量。

最小二乘法在实践中很常用，因为它有解析解，可以直接通过数学公式计算出最优的回归系数，不需要迭代优化。但是，对于大规模数据集或复杂模型，可能会有其他更高级的优化技术被用于最小化损失函数。

##### 梯度下降

简单线性回归中，梯度下降是一种优化算法，用于最小化损失函数，以找到最佳的回归系数 \(b\) 和 \(w_1\)。梯度下降的目标是沿着损失函数梯度的反方向逐步调整参数，直至达到损失函数的最小值。

对于简单线性回归的最小二乘法损失函数：

\[ \text{Loss} = \sum_{i=1}^{n} (y_i - (b + w_1x_i))^2 \]

梯度下降的步骤如下：

1. **初始化参数 \(b\) 和 \(w_1\)**：可以随机初始化或使用零值开始。

2. **计算损失函数关于参数的梯度**：对损失函数进行求导，得到关于 \(b\) 和 \(w_1\) 的梯度。

3. **更新参数**：沿着梯度的反方向调整参数值，更新 \(b\) 和 \(w_1\)。更新公式如下：
其中 \(\alpha\) 是学习率，控制每次参数更新的步长。

\[ b = b - \alpha \frac{\partial \text{Loss}}{\partial b} \]
\[ w_1 = w_1 - \alpha \frac{\partial \text{Loss}}{\partial w_1} \]

4. **重复迭代步骤2和步骤3**：直至达到指定的迭代次数或损失函数收敛到足够小的值。

关于梯度的具体计算会涉及损失函数的偏导数。对于最小二乘法的简单线性回归，梯度计算如下：

\[
\frac{\partial \text{Loss}}{\partial b} = -2 \sum_{i=1}^{n} (y_i - (b + w_1x_i))
\]

\[
\frac{\partial \text{Loss}}{\partial w_1} = -2 \sum_{i=1}^{n} x_i(y_i - (b + w_1x_i))
\]

梯度下降算法中的学习率 \(\alpha\) 的选择很重要。过小的学习率可能导致收敛速度缓慢，而过大的学习率可能导致震荡甚至无法收敛。所以，选择一个合适的学习率是调整梯度下降算法的关键部分。

#### 逻辑回归

### 推荐算法

### 支持向量机

## 无监督学习

### K-Means(K均值聚类)

**训练过程**

第一步：选择k个点作为初始簇心

第二步：计算每个点到每个簇心的距离，将每个点划分到距离最近的簇中

第三步：跟新簇心位置

第四步：重复第二步和第三步，直到簇心位置基本不变

**性能评估**

聚类算法是无监督算法性能比较低需要完成性能评估

常用的评价方法

- 外部有效性评价
- 内部有效性评价
- 相关性测试评价

K-Means目标函数

假设数据集X包含n个数据点，需要划分k个类聚类中心的集合用U表示

\[ J=\sum_{c=1}^{k}\sum_{i=1}^{n}||x_i-u_c||^2 \]

**科学的确定k值**

1. 通过经验判断
2. 可视化数据人工判断
3. 通过肘法画出J关于k值的图可以明显看见拐点确定k值
**SSE(误差平方和)**\( \sum_{i=1}^n(y_i-\widehat{y}) \)其实和函数J是一样的

## 强化学习

## 深度学习

## 性能评估

### 准确率

准确率是衡量分类模型预测准确性的指标，它计算的是模型正确预测的样本数量与总样本数量的比例。

准确率的计算公式如下：

\[ \text{准确率} = \frac{\text{正确预测的样本数量}}{\text{总样本数量}} \times 100\% \]

如果有一个分类问题，有 \(N\) 个样本，模型成功预测了 \(M\) 个样本，其中 \(M\) 个样本的预测结果与实际结果相符，那么准确率就是 \( \frac{M}{N} \times 100\% \)。 

举个例子，假设有100个样本，分类模型正确预测了其中的80个样本，那么准确率为 \( \frac{80}{100} \times 100\% = 80\% \)。

### 精确率(Precision)和召回率(Recall)

二分类问题的混淆矩阵是一个 2x2 的矩阵，用于总结分类模型在预测过程中的性能。它展示了实际类别与模型预测类别之间的对应关系。

在混淆矩阵中，通常有四个关键的值：

- **真正例（True Positives，TP）：** 实际为正类且被模型预测为正类的样本数量。
- **真负例（True Negatives，TN）：** 实际为负类且被模型预测为负类的样本数量。
- **假正例（False Positives，FP）：** 实际为负类但被模型错误预测为正类的样本数量（误报）。
- **假负例（False Negatives，FN）：** 实际为正类但被模型错误预测为负类的样本数量（漏报）。

以混淆矩阵的形式呈现如下：

||预测值0|预测值1|
|----|----|----|
|真实值0|TN|FP|
|真实值1|FN|TP|

**精准率**

 \( precision = \frac {TP} {TP + FP} \)

所以，精准率就是“预测为正例的那些数据里预测正确的数据个数”

**召回率**

 \( recall = \frac {TP} {TP + FN} \)

所以，召回率就是“真实为正例的那些数据里预测正确的数据个数

****

**三元分类问题的混淆矩阵**

在多分类（大于两个类）问题中，假设我们要开发一个动物识别系统，来区分输入图片是猫，狗还是猪。给定分类器一堆动物图片，产生了如下结果混淆矩阵。

![三元分类](./images/屏幕截图%202023-12-05%20142957.png)

在混淆矩阵中，正确的分类样本（Actual label = Predicted label）分布在左上到右下的对角线上。其中，Accuracy的定义为分类正确（对角线上）的样本数与总样本数的比值。**Accuracy度量的是全局样本预测情况。而对于Precision和Recall而言，每个类都需要单独计算其Precision和Recall**

![三元分类和混淆矩阵](./images/屏幕截图%202023-12-05%20143412.png)

比如，对类别「猪」而言，其Precision和Recall分别为

\( \text{precision} = \frac {TP} {TP + FP} = \frac {20} {20+50} \)

\( \text{recall} = \frac {TP} {TP + FN} = \frac {20} {20+10} \)

### f1-score

F1 分数是综合评价分类模型性能的指标，特别适用于不平衡类别的数据集。它是精确率（Precision）和召回率（Recall）的调和平均数。

F1 分数的计算公式为：

\[ F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} \]

F1 分数的取值范围在 0 到 1 之间，越接近 1 表示模型在精确率和召回率上表现越好。这个指标适合于处理数据不平衡的情况，因为它同时考虑了 Precision 和 Recall。

F1 分数综合考虑了 Precision 和 Recall，对模型在不同类别上的预测性能进行整体评估。当 Precision 和 Recall 重要性相近或需要在两者之间取得平衡时，F1 分数尤其有用。